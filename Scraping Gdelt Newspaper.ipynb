{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords \n",
    "from nltk import pos_tag\n",
    "from nltk.tag import pos_tag\n",
    "\n",
    "import newspaper\n",
    "from newspaper import Article\n",
    "\n",
    "import string\n",
    "import json\n",
    "import operator\n",
    "from collections import Counter\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "import re\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "header = [\"GlobalEventId\", \"sqldate\", \"MonthYear\",\"Year\",\n",
    "               \"Actor1Code\",\"Actor1Name\",\"Actor1CountryCode\",\"Actor1KnownGroupCode\",\"Actor1Type1Code\",\n",
    "               \"Actor2Code\",\"Actor2Name\",\"Actor2CountryCode\",\"Actor2KnownGroupCode\",\"Actor2Type1Code\",\n",
    "               \"IsRootEvent\",\"EventCode\",\"EventBaseCode\",\"EventRootCode\",\n",
    "               \"QuadClass\",\"GoldsteinScale\",\"NumMentions\",\"NumSources\",\"NumArticles\",\"AvgTone\",\n",
    "               \"Actor1Geo_FullName\",\"Actor1Geo_CountryCode\",\"Actor1Geo_ADM1Code\",\"Actor1Geo_Lat\",\"Actor1Geo_Long\",\n",
    "               \"Actor2Geo_FullName\",\"Actor2Geo_CountryCode\",\"Actor2Geo_ADM1Code\",\"Actor2Geo_Lat\",\"Actor2Geo_Long\",\n",
    "               \"ActionGeo_FullName\",\"ActionGeo_CountryCode\",\"ActionGeo_ADM1Code\",\"ActionGeo_Lat\",\"ActionGeo_Long\",\n",
    "               \"SOURCEURL\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GlobalEventId</th>\n",
       "      <th>sqldate</th>\n",
       "      <th>MonthYear</th>\n",
       "      <th>Year</th>\n",
       "      <th>Actor1Code</th>\n",
       "      <th>Actor1Name</th>\n",
       "      <th>Actor1CountryCode</th>\n",
       "      <th>Actor1KnownGroupCode</th>\n",
       "      <th>Actor1Type1Code</th>\n",
       "      <th>Actor2Code</th>\n",
       "      <th>...</th>\n",
       "      <th>Actor2Geo_CountryCode</th>\n",
       "      <th>Actor2Geo_ADM1Code</th>\n",
       "      <th>Actor2Geo_Lat</th>\n",
       "      <th>Actor2Geo_Long</th>\n",
       "      <th>ActionGeo_FullName</th>\n",
       "      <th>ActionGeo_CountryCode</th>\n",
       "      <th>ActionGeo_ADM1Code</th>\n",
       "      <th>ActionGeo_Lat</th>\n",
       "      <th>ActionGeo_Long</th>\n",
       "      <th>SOURCEURL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1144227</th>\n",
       "      <td>591789438</td>\n",
       "      <td>20161023</td>\n",
       "      <td>201610</td>\n",
       "      <td>2016</td>\n",
       "      <td>ITA</td>\n",
       "      <td>ITALY</td>\n",
       "      <td>ITA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>BUS</td>\n",
       "      <td>...</td>\n",
       "      <td>IT</td>\n",
       "      <td>IT09</td>\n",
       "      <td>45.4667</td>\n",
       "      <td>9.2</td>\n",
       "      <td>Milan, Lombardia, Italy</td>\n",
       "      <td>IT</td>\n",
       "      <td>IT09</td>\n",
       "      <td>45.4667</td>\n",
       "      <td>9.2000</td>\n",
       "      <td>http://energy.einnews.com/article/350625569/live</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1012115</th>\n",
       "      <td>580546591</td>\n",
       "      <td>20160918</td>\n",
       "      <td>201609</td>\n",
       "      <td>2016</td>\n",
       "      <td>CHR</td>\n",
       "      <td>CHRISTIAN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CVL</td>\n",
       "      <td>...</td>\n",
       "      <td>GM</td>\n",
       "      <td>GM16</td>\n",
       "      <td>52.5167</td>\n",
       "      <td>13.4</td>\n",
       "      <td>Berlin, Berlin, Germany</td>\n",
       "      <td>GM</td>\n",
       "      <td>GM16</td>\n",
       "      <td>52.5167</td>\n",
       "      <td>13.4000</td>\n",
       "      <td>http://news.webindia123.com/news/Articles/Worl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1199592</th>\n",
       "      <td>596671889</td>\n",
       "      <td>20161107</td>\n",
       "      <td>201611</td>\n",
       "      <td>2016</td>\n",
       "      <td>MED</td>\n",
       "      <td>WRITER</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>MED</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Massachusetts, United States</td>\n",
       "      <td>US</td>\n",
       "      <td>USMA</td>\n",
       "      <td>42.2373</td>\n",
       "      <td>-71.5314</td>\n",
       "      <td>http://www.jta.org/2016/11/07/news-opinion/pol...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1251725</th>\n",
       "      <td>601872165</td>\n",
       "      <td>20161123</td>\n",
       "      <td>201611</td>\n",
       "      <td>2016</td>\n",
       "      <td>GHA</td>\n",
       "      <td>GHANA</td>\n",
       "      <td>GHA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>GHAGOV</td>\n",
       "      <td>...</td>\n",
       "      <td>SY</td>\n",
       "      <td>SY</td>\n",
       "      <td>35.0000</td>\n",
       "      <td>38.0</td>\n",
       "      <td>Syria</td>\n",
       "      <td>SY</td>\n",
       "      <td>SY</td>\n",
       "      <td>35.0000</td>\n",
       "      <td>38.0000</td>\n",
       "      <td>http://www.myjoyonline.com/politics/2016/Novem...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4 rows Ã— 40 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         GlobalEventId   sqldate  MonthYear  Year Actor1Code Actor1Name  \\\n",
       "1144227      591789438  20161023     201610  2016        ITA      ITALY   \n",
       "1012115      580546591  20160918     201609  2016        CHR  CHRISTIAN   \n",
       "1199592      596671889  20161107     201611  2016        MED     WRITER   \n",
       "1251725      601872165  20161123     201611  2016        GHA      GHANA   \n",
       "\n",
       "        Actor1CountryCode Actor1KnownGroupCode Actor1Type1Code Actor2Code  \\\n",
       "1144227               ITA                  NaN             NaN        BUS   \n",
       "1012115               NaN                  NaN             NaN        CVL   \n",
       "1199592               NaN                  NaN             MED        NaN   \n",
       "1251725               GHA                  NaN             NaN     GHAGOV   \n",
       "\n",
       "                               ...                          \\\n",
       "1144227                        ...                           \n",
       "1012115                        ...                           \n",
       "1199592                        ...                           \n",
       "1251725                        ...                           \n",
       "\n",
       "        Actor2Geo_CountryCode Actor2Geo_ADM1Code Actor2Geo_Lat Actor2Geo_Long  \\\n",
       "1144227                    IT               IT09       45.4667            9.2   \n",
       "1012115                    GM               GM16       52.5167           13.4   \n",
       "1199592                   NaN                NaN           NaN            NaN   \n",
       "1251725                    SY                 SY       35.0000           38.0   \n",
       "\n",
       "                   ActionGeo_FullName  ActionGeo_CountryCode  \\\n",
       "1144227       Milan, Lombardia, Italy                     IT   \n",
       "1012115       Berlin, Berlin, Germany                     GM   \n",
       "1199592  Massachusetts, United States                     US   \n",
       "1251725                         Syria                     SY   \n",
       "\n",
       "         ActionGeo_ADM1Code  ActionGeo_Lat  ActionGeo_Long  \\\n",
       "1144227                IT09        45.4667          9.2000   \n",
       "1012115                GM16        52.5167         13.4000   \n",
       "1199592                USMA        42.2373        -71.5314   \n",
       "1251725                  SY        35.0000         38.0000   \n",
       "\n",
       "                                                 SOURCEURL  \n",
       "1144227   http://energy.einnews.com/article/350625569/live  \n",
       "1012115  http://news.webindia123.com/news/Articles/Worl...  \n",
       "1199592  http://www.jta.org/2016/11/07/news-opinion/pol...  \n",
       "1251725  http://www.myjoyonline.com/politics/2016/Novem...  \n",
       "\n",
       "[4 rows x 40 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#ubah path_clean sesuai tempat clean di lokal\n",
    "path_clean = 'E:/digitalent/2016-clean.csv'\n",
    "#path_clean = 'E:/digitalent-project/csv/2017-clean.csv'\n",
    "data_gdelt = pd.read_csv(path_clean, delimiter = ',', encoding = \"ISO-8859-1\", names=header)\n",
    "data_gdelt.sample(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inisialisasi Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inisialisasi Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = nltk.stem.PorterStemmer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inisialisasi dict res untuk menampung result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fungsi Scrapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sebelum file json ada\n",
    "def scrap_article(gid, source):\n",
    "    start = time.time()\n",
    "    article = Article(source)\n",
    "    article.download()\n",
    "    article.parse()\n",
    "    article.nlp()\n",
    "    \n",
    "    #LOWERCASE\n",
    "    txt = article.text.lower()\n",
    "    \n",
    "    #change nonword to space   \n",
    "    txt_clean = re.sub(r'[^a-zA-Z]', ' ', txt)\n",
    "    \n",
    "    #TOKENISASI , STEM, HAPUS PROPERNOUN, DAN HAPUS STOPWORD\n",
    "    tagged_sentence = nltk.pos_tag(txt_clean.split())\n",
    "    \n",
    "    filtered_sentence=[]\n",
    "    for initword, tag in tagged_sentence:\n",
    "        word = ps.stem(initword)\n",
    "        if len(word)>1 and (word not in stop_words) and tag != 'NNP' and tag != 'NNPS':\n",
    "            filtered_sentence.append(word)\n",
    "\n",
    "    #print(\"TOKENS\",type(filtered_sentence))\n",
    "    \n",
    "    #COUNT VECTOR: hitung kemunculan tiap kata\n",
    "    gid_str = str(gid)\n",
    "    val = dict((Counter(filtered_sentence)))\n",
    "    \n",
    "    #masukkan hasil ke dict res dengan id : gid_str dan value: val\n",
    "    res[gid_str] = val\n",
    "    end = time.time()\n",
    "    waktu = end-start\n",
    "    #print(waktu)\n",
    "    return val\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WARNING!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#jalankan jika sudah ada file json\n",
    "def scrap_article(gid, source):\n",
    "    start = time.time()\n",
    "    article = Article(source)\n",
    "    article.download()\n",
    "    article.parse()\n",
    "    article.nlp()\n",
    "    \n",
    "    #LOWERCASE\n",
    "    txt = article.text.lower()\n",
    "    \n",
    "    #change nonword to space   \n",
    "    txt_clean = re.sub(r'[^a-zA-Z]', ' ', txt)\n",
    "    \n",
    "    #TOKENISASI , STEM, HAPUS PROPERNOUN, DAN HAPUS STOPWORD\n",
    "    tagged_sentence = nltk.pos_tag(txt_clean.split())\n",
    "    \n",
    "    filtered_sentence=[]\n",
    "    for initword, tag in tagged_sentence:\n",
    "        word = ps.stem(initword)\n",
    "        if len(word)>1 and (word not in stop_words) and tag != 'NNP' and tag != 'NNPS':\n",
    "            filtered_sentence.append(word)\n",
    "\n",
    "    #print(\"TOKENS\",type(filtered_sentence))\n",
    "    \n",
    "    #COUNT VECTOR: hitung kemunculan tiap kata\n",
    "    gid_str = str(gid)\n",
    "    val = dict((Counter(filtered_sentence)))\n",
    "    \n",
    "    #masukkan hasil ke dict res dengan id : gid_str dan value: val\n",
    "    res[gid_str] = val\n",
    "    \n",
    "    #update JSON file\n",
    "    a_dict = {gid_str: val}\n",
    "\n",
    "    with open('data.json') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    data.update(a_dict)\n",
    "\n",
    "    with open('data.json', 'w') as f:\n",
    "        json.dump(data, f)\n",
    "    \n",
    "    end = time.time()\n",
    "    waktu = end-start\n",
    "    #print(waktu)\n",
    "    return val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Contoh 1 url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'close': 1,\n",
       " 'rochest': 6,\n",
       " 'man': 2,\n",
       " 'accus': 2,\n",
       " 'isi': 10,\n",
       " 'sympath': 2,\n",
       " 'plan': 7,\n",
       " 'kill': 7,\n",
       " 'patron': 3,\n",
       " 'restaur': 3,\n",
       " 'bar': 3,\n",
       " 'new': 10,\n",
       " 'year': 13,\n",
       " 'eve': 8,\n",
       " 'feder': 6,\n",
       " 'author': 6,\n",
       " 'alleg': 3,\n",
       " 'angi': 1,\n",
       " 'nassar': 1,\n",
       " 'gari': 1,\n",
       " 'craig': 1,\n",
       " 'emanuel': 4,\n",
       " 'lutchman': 26,\n",
       " 'photo': 1,\n",
       " 'provid': 3,\n",
       " 'attack': 5,\n",
       " 'say': 5,\n",
       " 'could': 3,\n",
       " 'gruesom': 1,\n",
       " 'assault': 2,\n",
       " 'islam': 4,\n",
       " 'state': 9,\n",
       " 'arm': 1,\n",
       " 'knive': 2,\n",
       " 'machet': 2,\n",
       " 'take': 1,\n",
       " 'life': 1,\n",
       " 'old': 1,\n",
       " 'allegedli': 6,\n",
       " 'said': 11,\n",
       " 'tuesday': 1,\n",
       " 'hi': 9,\n",
       " 'appar': 1,\n",
       " 'plot': 4,\n",
       " 'merchant': 3,\n",
       " 'grill': 2,\n",
       " 'road': 1,\n",
       " 'problem': 1,\n",
       " 'fbi': 12,\n",
       " 'joint': 2,\n",
       " 'terror': 3,\n",
       " 'task': 2,\n",
       " 'forc': 2,\n",
       " 'wednesday': 2,\n",
       " 'arrest': 8,\n",
       " 'charg': 3,\n",
       " 'offer': 1,\n",
       " 'materi': 3,\n",
       " 'support': 2,\n",
       " 'terrorist': 5,\n",
       " 'appear': 3,\n",
       " 'court': 6,\n",
       " 'thursday': 10,\n",
       " 'morn': 1,\n",
       " 'media': 2,\n",
       " 'wa': 10,\n",
       " 'alert': 1,\n",
       " 'advanc': 1,\n",
       " 'schedul': 3,\n",
       " 'return': 2,\n",
       " 'jan': 1,\n",
       " 'detain': 1,\n",
       " 'populac': 1,\n",
       " 'alreadi': 1,\n",
       " 'fear': 1,\n",
       " 'impact': 1,\n",
       " 'propaganda': 2,\n",
       " 'extrem': 1,\n",
       " 'fundamentalist': 1,\n",
       " 'muslim': 2,\n",
       " 'mental': 5,\n",
       " 'unstabl': 3,\n",
       " 'enter': 1,\n",
       " 'nation': 1,\n",
       " 'convers': 2,\n",
       " 'reach': 1,\n",
       " 'network': 1,\n",
       " 'citi': 1,\n",
       " 'cancel': 1,\n",
       " 'firework': 1,\n",
       " 'wake': 1,\n",
       " 'earlier': 2,\n",
       " 'decemb': 2,\n",
       " 'pizza': 1,\n",
       " 'shop': 1,\n",
       " 'owner': 2,\n",
       " 'mufid': 1,\n",
       " 'elfgeeh': 5,\n",
       " 'plead': 2,\n",
       " 'guilti': 2,\n",
       " 'tri': 2,\n",
       " 'recruit': 2,\n",
       " 'two': 1,\n",
       " 'men': 1,\n",
       " 'also': 5,\n",
       " 'known': 1,\n",
       " 'isil': 3,\n",
       " 'public': 1,\n",
       " 'record': 1,\n",
       " 'avail': 3,\n",
       " 'knew': 1,\n",
       " 'reveal': 1,\n",
       " 'may': 4,\n",
       " 'particularli': 1,\n",
       " 'suscept': 1,\n",
       " 'influenc': 1,\n",
       " 'ha': 5,\n",
       " 'har': 1,\n",
       " 'social': 1,\n",
       " 'seek': 1,\n",
       " 'establish': 2,\n",
       " 'foothold': 1,\n",
       " 'middl': 1,\n",
       " 'east': 1,\n",
       " 'caliph': 1,\n",
       " 'mid': 1,\n",
       " 'januari': 1,\n",
       " 'domest': 1,\n",
       " 'violenc': 1,\n",
       " 'involv': 2,\n",
       " 'girlfriend': 1,\n",
       " 'thi': 8,\n",
       " 'menac': 1,\n",
       " 'woman': 1,\n",
       " 'specif': 1,\n",
       " 'misdemeanor': 1,\n",
       " 'incid': 2,\n",
       " 'immedi': 2,\n",
       " 'self': 1,\n",
       " 'profess': 1,\n",
       " 'convert': 1,\n",
       " 'crimin': 2,\n",
       " 'histori': 1,\n",
       " 'date': 1,\n",
       " 'back': 2,\n",
       " 'approxim': 2,\n",
       " 'well': 1,\n",
       " 'previou': 1,\n",
       " 'hygien': 3,\n",
       " 'special': 2,\n",
       " 'agent': 2,\n",
       " 'timothi': 1,\n",
       " 'klapec': 2,\n",
       " 'wrote': 2,\n",
       " 'dec': 2,\n",
       " 'affidavit': 3,\n",
       " 'polic': 1,\n",
       " 'execut': 1,\n",
       " 'someon': 1,\n",
       " 'psycholog': 1,\n",
       " 'hurt': 1,\n",
       " 'themselv': 1,\n",
       " 'detail': 1,\n",
       " 'serv': 1,\n",
       " 'prison': 1,\n",
       " 'time': 2,\n",
       " 'robberi': 1,\n",
       " 'convict': 1,\n",
       " 'quick': 1,\n",
       " 'look': 1,\n",
       " 'know': 1,\n",
       " 'far': 1,\n",
       " 'suspect': 1,\n",
       " 'paper': 4,\n",
       " 'show': 1,\n",
       " 'util': 1,\n",
       " 'three': 1,\n",
       " 'inform': 10,\n",
       " 'build': 1,\n",
       " 'case': 1,\n",
       " 'one': 3,\n",
       " 'point': 1,\n",
       " 'consid': 1,\n",
       " 'abandon': 1,\n",
       " 'decid': 2,\n",
       " 'anoth': 1,\n",
       " 'told': 5,\n",
       " 'let': 1,\n",
       " 'oper': 4,\n",
       " 'upset': 1,\n",
       " 'continu': 1,\n",
       " 'troubl': 1,\n",
       " 'health': 1,\n",
       " 'past': 2,\n",
       " 'rais': 1,\n",
       " 'question': 2,\n",
       " 'seriou': 2,\n",
       " 'whether': 1,\n",
       " 'push': 1,\n",
       " 'toward': 1,\n",
       " 'offici': 2,\n",
       " 'prosecutor': 3,\n",
       " 'declin': 2,\n",
       " 'comment': 1,\n",
       " 'beyond': 1,\n",
       " 'statement': 1,\n",
       " 'news': 4,\n",
       " 'releas': 2,\n",
       " 'thwart': 3,\n",
       " 'intent': 4,\n",
       " 'civilian': 3,\n",
       " 'adam': 1,\n",
       " 'cohen': 1,\n",
       " 'discuss': 3,\n",
       " 'would': 4,\n",
       " 'answer': 1,\n",
       " 'plea': 2,\n",
       " 'like': 1,\n",
       " 'proof': 1,\n",
       " 'earli': 1,\n",
       " 'brother': 1,\n",
       " 'locat': 2,\n",
       " 'oversea': 1,\n",
       " 'express': 1,\n",
       " 'hatr': 1,\n",
       " 'everyth': 1,\n",
       " 'american': 2,\n",
       " 'make': 1,\n",
       " 'hijra': 2,\n",
       " 'leav': 1,\n",
       " 'america': 2,\n",
       " 'mean': 1,\n",
       " 'migrat': 1,\n",
       " 'term': 1,\n",
       " 'travel': 1,\n",
       " 'join': 1,\n",
       " 'connect': 2,\n",
       " 'individu': 2,\n",
       " 'need': 1,\n",
       " 'prove': 2,\n",
       " 'whenev': 1,\n",
       " 'kuffar': 2,\n",
       " 'mettl': 1,\n",
       " 'arab': 1,\n",
       " 'word': 1,\n",
       " 'infidel': 1,\n",
       " 'suggest': 2,\n",
       " 'use': 2,\n",
       " 'bomb': 1,\n",
       " 'insid': 1,\n",
       " 'club': 1,\n",
       " 'kidnap': 1,\n",
       " 'coupl': 1,\n",
       " 'peopl': 1,\n",
       " 'accord': 2,\n",
       " 'identifi': 4,\n",
       " 'potenti': 2,\n",
       " 'target': 1,\n",
       " 'co': 1,\n",
       " 'john': 3,\n",
       " 'page': 1,\n",
       " 'went': 1,\n",
       " 'walmart': 1,\n",
       " 'hudson': 1,\n",
       " 'avenu': 1,\n",
       " 'bought': 2,\n",
       " 'duct': 1,\n",
       " 'tape': 1,\n",
       " 'ammonia': 1,\n",
       " 'latex': 1,\n",
       " 'glove': 1,\n",
       " 'zip': 1,\n",
       " 'tie': 1,\n",
       " 'abduct': 1,\n",
       " 'money': 1,\n",
       " 'later': 1,\n",
       " 'go': 2,\n",
       " 'get': 1,\n",
       " 'real': 1,\n",
       " 'lord': 1,\n",
       " 'gotta': 1,\n",
       " 'grab': 1,\n",
       " 'somebodi': 1,\n",
       " 'live': 2,\n",
       " 'vehicl': 1,\n",
       " 'car': 1,\n",
       " 'befor': 1,\n",
       " 'made': 1,\n",
       " 'video': 1,\n",
       " 'mobil': 1,\n",
       " 'telephon': 1,\n",
       " 'pledg': 1,\n",
       " 'allegi': 1,\n",
       " 'claim': 1,\n",
       " 'respons': 1,\n",
       " 'prais': 1,\n",
       " 'law': 5,\n",
       " 'enforc': 5,\n",
       " 'bloodi': 1,\n",
       " 'stop': 2,\n",
       " 'complaint': 1,\n",
       " 'part': 1,\n",
       " 'attempt': 1,\n",
       " 'innoc': 2,\n",
       " 'name': 3,\n",
       " 'organ': 2,\n",
       " 'assist': 2,\n",
       " 'attorney': 3,\n",
       " 'gener': 2,\n",
       " 'carlin': 2,\n",
       " 'thank': 1,\n",
       " 'abl': 1,\n",
       " 'interven': 1,\n",
       " 'deadli': 1,\n",
       " 'william': 1,\n",
       " 'hochul': 1,\n",
       " 'jr': 1,\n",
       " 'prosecut': 1,\n",
       " 'underscor': 1,\n",
       " 'threat': 1,\n",
       " 'even': 1,\n",
       " 'upstat': 1,\n",
       " 'york': 2,\n",
       " 'demonstr': 1,\n",
       " 'determin': 1,\n",
       " 'ani': 1,\n",
       " 'caus': 1,\n",
       " 'harm': 1,\n",
       " 'onc': 1,\n",
       " 'lone': 2,\n",
       " 'wolf': 1,\n",
       " 'encourag': 2,\n",
       " 'sen': 1,\n",
       " 'charl': 1,\n",
       " 'schumer': 1,\n",
       " 'clear': 1,\n",
       " 'focus': 1,\n",
       " 'disaffect': 1,\n",
       " 'act': 2,\n",
       " 'wolv': 1,\n",
       " 'commit': 1,\n",
       " 'evil': 1,\n",
       " 'second': 1,\n",
       " 'month': 1,\n",
       " 'last': 1,\n",
       " 'troop': 1,\n",
       " 'agreement': 1,\n",
       " 'sentenc': 2,\n",
       " 'march': 1,\n",
       " 'interview': 1,\n",
       " 'warner': 1,\n",
       " 'cabl': 1,\n",
       " 'afternoon': 1,\n",
       " 'gov': 1,\n",
       " 'andrew': 1,\n",
       " 'cuomo': 1,\n",
       " 'good': 1,\n",
       " 'work': 1,\n",
       " 'signific': 1,\n",
       " 'experi': 1,\n",
       " 'fight': 2,\n",
       " 'although': 1,\n",
       " 'becom': 1,\n",
       " 'difficult': 1,\n",
       " 'risen': 1,\n",
       " 'challeng': 2,\n",
       " 'call': 1,\n",
       " 'citizen': 2,\n",
       " 'vigil': 1,\n",
       " 'report': 2,\n",
       " 'suspici': 1,\n",
       " 'activ': 1,\n",
       " 'someth': 3,\n",
       " 'ongo': 1,\n",
       " 'everi': 1,\n",
       " 'must': 1,\n",
       " 'remain': 1,\n",
       " 'dilig': 1,\n",
       " 'see': 1,\n",
       " 'gcraig': 1,\n",
       " 'gannett': 1,\n",
       " 'com': 1,\n",
       " 'includ': 1,\n",
       " 'staff': 1,\n",
       " 'writer': 1,\n",
       " 'brian': 1,\n",
       " 'sharp': 1,\n",
       " 'meaghan': 1,\n",
       " 'mcdermott': 1,\n",
       " 'jon': 1,\n",
       " 'hand': 1,\n",
       " 'sean': 1,\n",
       " 'lahman': 1,\n",
       " 'read': 1,\n",
       " 'share': 1,\n",
       " 'stori': 1,\n",
       " 'http': 1,\n",
       " 'rocn': 1,\n",
       " 'ws': 1,\n",
       " 'dbig': 1}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample=data_gdelt['SOURCEURL'][0]\n",
    "sample\n",
    "sample_ID = data_gdelt['GlobalEventId'][0]\n",
    "scrap_article(sample_ID, sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\suci.lestari\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proses Menghasilkan Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0.2 percent, 1 position, 0.006133967805800569 per second \n",
      "gagal http://www.navytimes.com/story/news/nation-now/2015/12/31/ny-man-charged-islamic-state-new-years-eve-attack/78130922/\n",
      " 0.4 percent, 2 position, 0.006040221865721991 per second \n",
      "gagal http://www.navytimes.com/story/military/2015/12/31/irans-president-orders-stepped-up-missile-production/78147134/\n",
      " 0.6 percent, 3 position, 0.005994024473625889 per second \n",
      "gagal http://newsok.com/police-man-accused-of-road-rage-shooting-stayed-on-scene/article/feed/943163\n",
      " 0.8 percent, 4 position, 0.005777073556630821 per second \n",
      "gagal http://www.13wmaz.com/story/news/local/macon/2015/12/31/two-wanted-for-robberies-in-four-states/78149932/\n",
      " 1.0 percent, 5 position, 0.005596975928353311 per second \n",
      "waktu 18.09385848045349\n"
     ]
    }
   ],
   "source": [
    "start2=time.time()\n",
    "progress = 0\n",
    "lenart = 5\n",
    "for x in range(5):\n",
    "    global start\n",
    "    global progress\n",
    "    progress += 1\n",
    "    try:\n",
    "        sample_=data_gdelt['SOURCEURL'][x]\n",
    "        sample_ID_ = data_gdelt['GlobalEventId'][x]\n",
    "        aaa = scrap_article(sample_ID_, sample_)\n",
    "        #print(aaa, type(aaa))\n",
    "    except:\n",
    "        print('gagal', sample_)\n",
    "        pass\n",
    "    end = time.time()\n",
    "    print('\\r {} percent, {} position, {} per second '.format(str(float(progress / lenart)), str(progress), (1 / (end - start)))) #lets us see how much time is \n",
    "end2 = time.time()\n",
    "\n",
    "print('waktu',end2-start2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(aaa))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1392383"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_gdelt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write hasil ke json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tidak dijalankan jika data.json sudah ada\n",
    "with open('data.json', 'w') as fp:\n",
    "    json.dump(res, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transform Count Vect to Tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### read json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data.json', 'r') as fp:\n",
    "    data_counter = json.load(fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### add event ID and bag of words to different array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "eventID = []\n",
    "DictVector = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k,v in data_counter.items():\n",
    "    eventID.append(k)\n",
    "    DictVector.append(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### vectorize bag of words from all news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = DictVectorizer()\n",
    "DictArray = vec.fit_transform(DictVector).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### transform to TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 0)\t0.017039954414774926\n",
      "  (0, 1)\t0.017039954414774926\n",
      "  (0, 2)\t0.017039954414774926\n",
      "  (0, 3)\t0.03407990882954985\n",
      "  (0, 4)\t0.03407990882954985\n",
      "  (0, 5)\t0.03407990882954985\n",
      "  (0, 6)\t0.017039954414774926\n",
      "  (0, 7)\t0.017039954414774926\n",
      "  (0, 8)\t0.017039954414774926\n",
      "  (0, 9)\t0.05111986324432477\n",
      "  (0, 10)\t0.017039954414774926\n",
      "  (0, 11)\t0.03407990882954985\n",
      "  (0, 12)\t0.017039954414774926\n",
      "  (0, 13)\t0.017039954414774926\n",
      "  (0, 14)\t0.05111986324432477\n",
      "  (0, 15)\t0.10223972648864954\n",
      "  (0, 16)\t0.017039954414774926\n",
      "  (0, 17)\t0.017039954414774926\n",
      "  (0, 18)\t0.08519977207387462\n",
      "  (0, 19)\t0.017039954414774926\n",
      "  (0, 20)\t0.03407990882954985\n",
      "  (0, 21)\t0.03407990882954985\n",
      "  (0, 22)\t0.017039954414774926\n",
      "  (0, 23)\t0.017039954414774926\n",
      "  (0, 24)\t0.017039954414774926\n",
      "  :\t:\n",
      "  (0, 375)\t0.017039954414774926\n",
      "  (0, 376)\t0.017039954414774926\n",
      "  (0, 377)\t0.017039954414774926\n",
      "  (0, 378)\t0.17039954414774924\n",
      "  (0, 379)\t0.017039954414774926\n",
      "  (0, 380)\t0.017039954414774926\n",
      "  (0, 381)\t0.017039954414774926\n",
      "  (0, 382)\t0.03407990882954985\n",
      "  (0, 383)\t0.017039954414774926\n",
      "  (0, 384)\t0.017039954414774926\n",
      "  (0, 385)\t0.017039954414774926\n",
      "  (0, 386)\t0.017039954414774926\n",
      "  (0, 387)\t0.017039954414774926\n",
      "  (0, 388)\t0.017039954414774926\n",
      "  (0, 389)\t0.017039954414774926\n",
      "  (0, 390)\t0.017039954414774926\n",
      "  (0, 391)\t0.017039954414774926\n",
      "  (0, 392)\t0.017039954414774926\n",
      "  (0, 393)\t0.0681598176590997\n",
      "  (0, 394)\t0.017039954414774926\n",
      "  (0, 395)\t0.03407990882954985\n",
      "  (0, 396)\t0.017039954414774926\n",
      "  (0, 397)\t0.221519407392074\n",
      "  (0, 398)\t0.03407990882954985\n",
      "  (0, 399)\t0.017039954414774926\n",
      "[[ 1.  1.  1.  2.  2.  2.  1.  1.  1.  3.  1.  2.  1.  1.  3.  6.  1.  1.\n",
      "   5.  1.  2.  2.  1.  1.  1.  1.  1.  1.  1.  3.  2.  1.  1.  8.  2.  2.\n",
      "   5.  1.  3.  6.  3.  1.  2.  3.  1.  1.  1.  1.  1.  2.  1.  1.  1.  1.\n",
      "   1.  1.  1.  1.  2.  1.  1.  2.  3.  1.  1.  2.  3.  1.  1.  1.  1.  1.\n",
      "   1.  1.  1.  1.  1.  2.  1.  1.  2.  1.  1.  3.  1.  6.  1.  2.  1.  1.\n",
      "   1.  1.  2.  2.  2.  2.  1.  1.  1.  1.  1.  1.  1.  3.  1.  1.  1.  2.\n",
      "   1.  5.  4.  2.  5.  1.  2.  8.  1.  1.  1.  1.  1.  1.  1.  1.  1. 12.\n",
      "   1.  6.  2.  1.  1.  1.  2.  1.  1.  1.  1.  2.  1.  1.  1.  2.  1.  1.\n",
      "   1.  1.  2.  1.  2.  5.  1.  1.  1.  1.  1.  9.  2.  1.  1.  1.  1.  1.\n",
      "   3.  4.  2.  1.  2.  1.  2.  1.  1. 10.  2.  1.  4.  1.  1.  2. 10.  3.\n",
      "   4.  1.  1.  3.  1.  2.  1.  1.  1.  7.  2.  1.  2.  1.  1.  2.  1.  1.\n",
      "   1.  1.  5.  1.  1.  1.  1.  2.  2.  2.  1.  1. 26.  2.  1.  1.  2.  1.\n",
      "   3.  4.  1.  1.  1.  2.  1.  1.  5.  3.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "   1.  1.  2.  1.  3.  1.  1.  1.  1. 10.  4.  1.  2.  1.  1.  3.  1.  4.\n",
      "   2.  1.  2.  1.  4.  1.  1.  2.  3.  1.  1.  1.  7.  2.  2.  1.  4.  1.\n",
      "   1.  1.  2.  1.  1.  1.  1.  1.  1.  2.  1.  3.  2.  3.  1.  1.  1.  2.\n",
      "   1.  1.  1.  1.  1.  1.  2.  2.  1.  2.  1.  3.  2.  1.  1.  1.  1.  6.\n",
      "   1. 11.  5.  3.  1.  1.  1.  1.  1.  1.  1.  2.  2.  1.  1.  1.  1.  1.\n",
      "   1.  1.  1.  1.  3.  2.  1.  1.  9.  1.  2.  1.  2.  2.  1.  1.  1.  2.\n",
      "   1.  1.  1.  2.  1.  1.  3.  5.  1.  1.  8.  1.  1. 10.  3.  1.  2.  1.\n",
      "   5.  1.  1.  2.  1.  1.  1.  1.  1.  3.  1.  1.  2.  1.  1.  1.  1.  1.\n",
      "  10.  1.  1.  1.  2.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  4.  1.  2.\n",
      "   1. 13.  2.  1.]]\n",
      "[[0.01703995 0.01703995 0.01703995 0.03407991 0.03407991 0.03407991\n",
      "  0.01703995 0.01703995 0.01703995 0.05111986 0.01703995 0.03407991\n",
      "  0.01703995 0.01703995 0.05111986 0.10223973 0.01703995 0.01703995\n",
      "  0.08519977 0.01703995 0.03407991 0.03407991 0.01703995 0.01703995\n",
      "  0.01703995 0.01703995 0.01703995 0.01703995 0.01703995 0.05111986\n",
      "  0.03407991 0.01703995 0.01703995 0.13631964 0.03407991 0.03407991\n",
      "  0.08519977 0.01703995 0.05111986 0.10223973 0.05111986 0.01703995\n",
      "  0.03407991 0.05111986 0.01703995 0.01703995 0.01703995 0.01703995\n",
      "  0.01703995 0.03407991 0.01703995 0.01703995 0.01703995 0.01703995\n",
      "  0.01703995 0.01703995 0.01703995 0.01703995 0.03407991 0.01703995\n",
      "  0.01703995 0.03407991 0.05111986 0.01703995 0.01703995 0.03407991\n",
      "  0.05111986 0.01703995 0.01703995 0.01703995 0.01703995 0.01703995\n",
      "  0.01703995 0.01703995 0.01703995 0.01703995 0.01703995 0.03407991\n",
      "  0.01703995 0.01703995 0.03407991 0.01703995 0.01703995 0.05111986\n",
      "  0.01703995 0.10223973 0.01703995 0.03407991 0.01703995 0.01703995\n",
      "  0.01703995 0.01703995 0.03407991 0.03407991 0.03407991 0.03407991\n",
      "  0.01703995 0.01703995 0.01703995 0.01703995 0.01703995 0.01703995\n",
      "  0.01703995 0.05111986 0.01703995 0.01703995 0.01703995 0.03407991\n",
      "  0.01703995 0.08519977 0.06815982 0.03407991 0.08519977 0.01703995\n",
      "  0.03407991 0.13631964 0.01703995 0.01703995 0.01703995 0.01703995\n",
      "  0.01703995 0.01703995 0.01703995 0.01703995 0.01703995 0.20447945\n",
      "  0.01703995 0.10223973 0.03407991 0.01703995 0.01703995 0.01703995\n",
      "  0.03407991 0.01703995 0.01703995 0.01703995 0.01703995 0.03407991\n",
      "  0.01703995 0.01703995 0.01703995 0.03407991 0.01703995 0.01703995\n",
      "  0.01703995 0.01703995 0.03407991 0.01703995 0.03407991 0.08519977\n",
      "  0.01703995 0.01703995 0.01703995 0.01703995 0.01703995 0.15335959\n",
      "  0.03407991 0.01703995 0.01703995 0.01703995 0.01703995 0.01703995\n",
      "  0.05111986 0.06815982 0.03407991 0.01703995 0.03407991 0.01703995\n",
      "  0.03407991 0.01703995 0.01703995 0.17039954 0.03407991 0.01703995\n",
      "  0.06815982 0.01703995 0.01703995 0.03407991 0.17039954 0.05111986\n",
      "  0.06815982 0.01703995 0.01703995 0.05111986 0.01703995 0.03407991\n",
      "  0.01703995 0.01703995 0.01703995 0.11927968 0.03407991 0.01703995\n",
      "  0.03407991 0.01703995 0.01703995 0.03407991 0.01703995 0.01703995\n",
      "  0.01703995 0.01703995 0.08519977 0.01703995 0.01703995 0.01703995\n",
      "  0.01703995 0.03407991 0.03407991 0.03407991 0.01703995 0.01703995\n",
      "  0.44303881 0.03407991 0.01703995 0.01703995 0.03407991 0.01703995\n",
      "  0.05111986 0.06815982 0.01703995 0.01703995 0.01703995 0.03407991\n",
      "  0.01703995 0.01703995 0.08519977 0.05111986 0.01703995 0.01703995\n",
      "  0.01703995 0.01703995 0.01703995 0.01703995 0.01703995 0.01703995\n",
      "  0.01703995 0.01703995 0.03407991 0.01703995 0.05111986 0.01703995\n",
      "  0.01703995 0.01703995 0.01703995 0.17039954 0.06815982 0.01703995\n",
      "  0.03407991 0.01703995 0.01703995 0.05111986 0.01703995 0.06815982\n",
      "  0.03407991 0.01703995 0.03407991 0.01703995 0.06815982 0.01703995\n",
      "  0.01703995 0.03407991 0.05111986 0.01703995 0.01703995 0.01703995\n",
      "  0.11927968 0.03407991 0.03407991 0.01703995 0.06815982 0.01703995\n",
      "  0.01703995 0.01703995 0.03407991 0.01703995 0.01703995 0.01703995\n",
      "  0.01703995 0.01703995 0.01703995 0.03407991 0.01703995 0.05111986\n",
      "  0.03407991 0.05111986 0.01703995 0.01703995 0.01703995 0.03407991\n",
      "  0.01703995 0.01703995 0.01703995 0.01703995 0.01703995 0.01703995\n",
      "  0.03407991 0.03407991 0.01703995 0.03407991 0.01703995 0.05111986\n",
      "  0.03407991 0.01703995 0.01703995 0.01703995 0.01703995 0.10223973\n",
      "  0.01703995 0.1874395  0.08519977 0.05111986 0.01703995 0.01703995\n",
      "  0.01703995 0.01703995 0.01703995 0.01703995 0.01703995 0.03407991\n",
      "  0.03407991 0.01703995 0.01703995 0.01703995 0.01703995 0.01703995\n",
      "  0.01703995 0.01703995 0.01703995 0.01703995 0.05111986 0.03407991\n",
      "  0.01703995 0.01703995 0.15335959 0.01703995 0.03407991 0.01703995\n",
      "  0.03407991 0.03407991 0.01703995 0.01703995 0.01703995 0.03407991\n",
      "  0.01703995 0.01703995 0.01703995 0.03407991 0.01703995 0.01703995\n",
      "  0.05111986 0.08519977 0.01703995 0.01703995 0.13631964 0.01703995\n",
      "  0.01703995 0.17039954 0.05111986 0.01703995 0.03407991 0.01703995\n",
      "  0.08519977 0.01703995 0.01703995 0.03407991 0.01703995 0.01703995\n",
      "  0.01703995 0.01703995 0.01703995 0.05111986 0.01703995 0.01703995\n",
      "  0.03407991 0.01703995 0.01703995 0.01703995 0.01703995 0.01703995\n",
      "  0.17039954 0.01703995 0.01703995 0.01703995 0.03407991 0.01703995\n",
      "  0.01703995 0.01703995 0.01703995 0.01703995 0.01703995 0.01703995\n",
      "  0.01703995 0.01703995 0.01703995 0.06815982 0.01703995 0.03407991\n",
      "  0.01703995 0.22151941 0.03407991 0.01703995]]\n"
     ]
    }
   ],
   "source": [
    "tfidf_transformer = TfidfTransformer(use_idf=False)\n",
    "tfidf_transform=tfidf_transformer.transform(DictArray, copy = True)\n",
    "print(tfidf_transform)\n",
    "print(DictArray)\n",
    "print(tfidf_transform.todense())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### header bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = vec.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('abandon', 0),\n",
       " ('abduct', 1),\n",
       " ('abl', 2),\n",
       " ('accord', 3),\n",
       " ('accus', 4),\n",
       " ('act', 5),\n",
       " ('activ', 6),\n",
       " ('adam', 7),\n",
       " ('advanc', 8),\n",
       " ('affidavit', 9),\n",
       " ('afternoon', 10),\n",
       " ('agent', 11),\n",
       " ('agreement', 12),\n",
       " ('alert', 13),\n",
       " ('alleg', 14),\n",
       " ('allegedli', 15),\n",
       " ('allegi', 16),\n",
       " ('alreadi', 17),\n",
       " ('also', 18),\n",
       " ('although', 19),\n",
       " ('america', 20),\n",
       " ('american', 21),\n",
       " ('ammonia', 22),\n",
       " ('andrew', 23),\n",
       " ('angi', 24),\n",
       " ('ani', 25),\n",
       " ('anoth', 26),\n",
       " ('answer', 27),\n",
       " ('appar', 28),\n",
       " ('appear', 29),\n",
       " ('approxim', 30),\n",
       " ('arab', 31),\n",
       " ('arm', 32),\n",
       " ('arrest', 33),\n",
       " ('assault', 34),\n",
       " ('assist', 35),\n",
       " ('attack', 36),\n",
       " ('attempt', 37),\n",
       " ('attorney', 38),\n",
       " ('author', 39),\n",
       " ('avail', 40),\n",
       " ('avenu', 41),\n",
       " ('back', 42),\n",
       " ('bar', 43),\n",
       " ('becom', 44),\n",
       " ('befor', 45),\n",
       " ('beyond', 46),\n",
       " ('bloodi', 47),\n",
       " ('bomb', 48),\n",
       " ('bought', 49),\n",
       " ('brian', 50),\n",
       " ('brother', 51),\n",
       " ('build', 52),\n",
       " ('cabl', 53),\n",
       " ('caliph', 54),\n",
       " ('call', 55),\n",
       " ('cancel', 56),\n",
       " ('car', 57),\n",
       " ('carlin', 58),\n",
       " ('case', 59),\n",
       " ('caus', 60),\n",
       " ('challeng', 61),\n",
       " ('charg', 62),\n",
       " ('charl', 63),\n",
       " ('citi', 64),\n",
       " ('citizen', 65),\n",
       " ('civilian', 66),\n",
       " ('claim', 67),\n",
       " ('clear', 68),\n",
       " ('close', 69),\n",
       " ('club', 70),\n",
       " ('co', 71),\n",
       " ('cohen', 72),\n",
       " ('com', 73),\n",
       " ('comment', 74),\n",
       " ('commit', 75),\n",
       " ('complaint', 76),\n",
       " ('connect', 77),\n",
       " ('consid', 78),\n",
       " ('continu', 79),\n",
       " ('convers', 80),\n",
       " ('convert', 81),\n",
       " ('convict', 82),\n",
       " ('could', 83),\n",
       " ('coupl', 84),\n",
       " ('court', 85),\n",
       " ('craig', 86),\n",
       " ('crimin', 87),\n",
       " ('cuomo', 88),\n",
       " ('date', 89),\n",
       " ('dbig', 90),\n",
       " ('deadli', 91),\n",
       " ('dec', 92),\n",
       " ('decemb', 93),\n",
       " ('decid', 94),\n",
       " ('declin', 95),\n",
       " ('demonstr', 96),\n",
       " ('detail', 97),\n",
       " ('detain', 98),\n",
       " ('determin', 99),\n",
       " ('difficult', 100),\n",
       " ('dilig', 101),\n",
       " ('disaffect', 102),\n",
       " ('discuss', 103),\n",
       " ('domest', 104),\n",
       " ('duct', 105),\n",
       " ('earli', 106),\n",
       " ('earlier', 107),\n",
       " ('east', 108),\n",
       " ('elfgeeh', 109),\n",
       " ('emanuel', 110),\n",
       " ('encourag', 111),\n",
       " ('enforc', 112),\n",
       " ('enter', 113),\n",
       " ('establish', 114),\n",
       " ('eve', 115),\n",
       " ('even', 116),\n",
       " ('everi', 117),\n",
       " ('everyth', 118),\n",
       " ('evil', 119),\n",
       " ('execut', 120),\n",
       " ('experi', 121),\n",
       " ('express', 122),\n",
       " ('extrem', 123),\n",
       " ('far', 124),\n",
       " ('fbi', 125),\n",
       " ('fear', 126),\n",
       " ('feder', 127),\n",
       " ('fight', 128),\n",
       " ('firework', 129),\n",
       " ('focus', 130),\n",
       " ('foothold', 131),\n",
       " ('forc', 132),\n",
       " ('fundamentalist', 133),\n",
       " ('gannett', 134),\n",
       " ('gari', 135),\n",
       " ('gcraig', 136),\n",
       " ('gener', 137),\n",
       " ('get', 138),\n",
       " ('girlfriend', 139),\n",
       " ('glove', 140),\n",
       " ('go', 141),\n",
       " ('good', 142),\n",
       " ('gotta', 143),\n",
       " ('gov', 144),\n",
       " ('grab', 145),\n",
       " ('grill', 146),\n",
       " ('gruesom', 147),\n",
       " ('guilti', 148),\n",
       " ('ha', 149),\n",
       " ('hand', 150),\n",
       " ('har', 151),\n",
       " ('harm', 152),\n",
       " ('hatr', 153),\n",
       " ('health', 154),\n",
       " ('hi', 155),\n",
       " ('hijra', 156),\n",
       " ('histori', 157),\n",
       " ('hochul', 158),\n",
       " ('http', 159),\n",
       " ('hudson', 160),\n",
       " ('hurt', 161),\n",
       " ('hygien', 162),\n",
       " ('identifi', 163),\n",
       " ('immedi', 164),\n",
       " ('impact', 165),\n",
       " ('incid', 166),\n",
       " ('includ', 167),\n",
       " ('individu', 168),\n",
       " ('infidel', 169),\n",
       " ('influenc', 170),\n",
       " ('inform', 171),\n",
       " ('innoc', 172),\n",
       " ('insid', 173),\n",
       " ('intent', 174),\n",
       " ('interven', 175),\n",
       " ('interview', 176),\n",
       " ('involv', 177),\n",
       " ('isi', 178),\n",
       " ('isil', 179),\n",
       " ('islam', 180),\n",
       " ('jan', 181),\n",
       " ('januari', 182),\n",
       " ('john', 183),\n",
       " ('join', 184),\n",
       " ('joint', 185),\n",
       " ('jon', 186),\n",
       " ('jr', 187),\n",
       " ('kidnap', 188),\n",
       " ('kill', 189),\n",
       " ('klapec', 190),\n",
       " ('knew', 191),\n",
       " ('knive', 192),\n",
       " ('know', 193),\n",
       " ('known', 194),\n",
       " ('kuffar', 195),\n",
       " ('lahman', 196),\n",
       " ('last', 197),\n",
       " ('later', 198),\n",
       " ('latex', 199),\n",
       " ('law', 200),\n",
       " ('leav', 201),\n",
       " ('let', 202),\n",
       " ('life', 203),\n",
       " ('like', 204),\n",
       " ('live', 205),\n",
       " ('locat', 206),\n",
       " ('lone', 207),\n",
       " ('look', 208),\n",
       " ('lord', 209),\n",
       " ('lutchman', 210),\n",
       " ('machet', 211),\n",
       " ('made', 212),\n",
       " ('make', 213),\n",
       " ('man', 214),\n",
       " ('march', 215),\n",
       " ('materi', 216),\n",
       " ('may', 217),\n",
       " ('mcdermott', 218),\n",
       " ('meaghan', 219),\n",
       " ('mean', 220),\n",
       " ('media', 221),\n",
       " ('men', 222),\n",
       " ('menac', 223),\n",
       " ('mental', 224),\n",
       " ('merchant', 225),\n",
       " ('mettl', 226),\n",
       " ('mid', 227),\n",
       " ('middl', 228),\n",
       " ('migrat', 229),\n",
       " ('misdemeanor', 230),\n",
       " ('mobil', 231),\n",
       " ('money', 232),\n",
       " ('month', 233),\n",
       " ('morn', 234),\n",
       " ('mufid', 235),\n",
       " ('muslim', 236),\n",
       " ('must', 237),\n",
       " ('name', 238),\n",
       " ('nassar', 239),\n",
       " ('nation', 240),\n",
       " ('need', 241),\n",
       " ('network', 242),\n",
       " ('new', 243),\n",
       " ('news', 244),\n",
       " ('offer', 245),\n",
       " ('offici', 246),\n",
       " ('old', 247),\n",
       " ('onc', 248),\n",
       " ('one', 249),\n",
       " ('ongo', 250),\n",
       " ('oper', 251),\n",
       " ('organ', 252),\n",
       " ('oversea', 253),\n",
       " ('owner', 254),\n",
       " ('page', 255),\n",
       " ('paper', 256),\n",
       " ('part', 257),\n",
       " ('particularli', 258),\n",
       " ('past', 259),\n",
       " ('patron', 260),\n",
       " ('peopl', 261),\n",
       " ('photo', 262),\n",
       " ('pizza', 263),\n",
       " ('plan', 264),\n",
       " ('plea', 265),\n",
       " ('plead', 266),\n",
       " ('pledg', 267),\n",
       " ('plot', 268),\n",
       " ('point', 269),\n",
       " ('polic', 270),\n",
       " ('populac', 271),\n",
       " ('potenti', 272),\n",
       " ('prais', 273),\n",
       " ('previou', 274),\n",
       " ('prison', 275),\n",
       " ('problem', 276),\n",
       " ('profess', 277),\n",
       " ('proof', 278),\n",
       " ('propaganda', 279),\n",
       " ('prosecut', 280),\n",
       " ('prosecutor', 281),\n",
       " ('prove', 282),\n",
       " ('provid', 283),\n",
       " ('psycholog', 284),\n",
       " ('public', 285),\n",
       " ('push', 286),\n",
       " ('question', 287),\n",
       " ('quick', 288),\n",
       " ('rais', 289),\n",
       " ('reach', 290),\n",
       " ('read', 291),\n",
       " ('real', 292),\n",
       " ('record', 293),\n",
       " ('recruit', 294),\n",
       " ('releas', 295),\n",
       " ('remain', 296),\n",
       " ('report', 297),\n",
       " ('respons', 298),\n",
       " ('restaur', 299),\n",
       " ('return', 300),\n",
       " ('reveal', 301),\n",
       " ('risen', 302),\n",
       " ('road', 303),\n",
       " ('robberi', 304),\n",
       " ('rochest', 305),\n",
       " ('rocn', 306),\n",
       " ('said', 307),\n",
       " ('say', 308),\n",
       " ('schedul', 309),\n",
       " ('schumer', 310),\n",
       " ('sean', 311),\n",
       " ('second', 312),\n",
       " ('see', 313),\n",
       " ('seek', 314),\n",
       " ('self', 315),\n",
       " ('sen', 316),\n",
       " ('sentenc', 317),\n",
       " ('seriou', 318),\n",
       " ('serv', 319),\n",
       " ('share', 320),\n",
       " ('sharp', 321),\n",
       " ('shop', 322),\n",
       " ('show', 323),\n",
       " ('signific', 324),\n",
       " ('social', 325),\n",
       " ('somebodi', 326),\n",
       " ('someon', 327),\n",
       " ('someth', 328),\n",
       " ('special', 329),\n",
       " ('specif', 330),\n",
       " ('staff', 331),\n",
       " ('state', 332),\n",
       " ('statement', 333),\n",
       " ('stop', 334),\n",
       " ('stori', 335),\n",
       " ('suggest', 336),\n",
       " ('support', 337),\n",
       " ('suscept', 338),\n",
       " ('suspect', 339),\n",
       " ('suspici', 340),\n",
       " ('sympath', 341),\n",
       " ('take', 342),\n",
       " ('tape', 343),\n",
       " ('target', 344),\n",
       " ('task', 345),\n",
       " ('telephon', 346),\n",
       " ('term', 347),\n",
       " ('terror', 348),\n",
       " ('terrorist', 349),\n",
       " ('thank', 350),\n",
       " ('themselv', 351),\n",
       " ('thi', 352),\n",
       " ('threat', 353),\n",
       " ('three', 354),\n",
       " ('thursday', 355),\n",
       " ('thwart', 356),\n",
       " ('tie', 357),\n",
       " ('time', 358),\n",
       " ('timothi', 359),\n",
       " ('told', 360),\n",
       " ('toward', 361),\n",
       " ('travel', 362),\n",
       " ('tri', 363),\n",
       " ('troop', 364),\n",
       " ('troubl', 365),\n",
       " ('tuesday', 366),\n",
       " ('two', 367),\n",
       " ('underscor', 368),\n",
       " ('unstabl', 369),\n",
       " ('upset', 370),\n",
       " ('upstat', 371),\n",
       " ('use', 372),\n",
       " ('util', 373),\n",
       " ('vehicl', 374),\n",
       " ('video', 375),\n",
       " ('vigil', 376),\n",
       " ('violenc', 377),\n",
       " ('wa', 378),\n",
       " ('wake', 379),\n",
       " ('walmart', 380),\n",
       " ('warner', 381),\n",
       " ('wednesday', 382),\n",
       " ('well', 383),\n",
       " ('went', 384),\n",
       " ('whenev', 385),\n",
       " ('whether', 386),\n",
       " ('william', 387),\n",
       " ('wolf', 388),\n",
       " ('wolv', 389),\n",
       " ('woman', 390),\n",
       " ('word', 391),\n",
       " ('work', 392),\n",
       " ('would', 393),\n",
       " ('writer', 394),\n",
       " ('wrote', 395),\n",
       " ('ws', 396),\n",
       " ('year', 397),\n",
       " ('york', 398),\n",
       " ('zip', 399)]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sort vocabulary by value (for header)\n",
    "header = sorted(vocabulary.items(), key=operator.itemgetter(1))\n",
    "header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "header_tfidf=[]\n",
    "for k,v in header:\n",
    "    header_tfidf.append(k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "matriks_tfidf = tfidf_transform.todense()\n",
    "df = pd.DataFrame(matriks_tfidf)\n",
    "df.to_csv(\"tfidf.csv\", index=False, header=header_tfidf, mode='w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
